from openai import OpenAI
import streamlit as st
import streamlit.components.v1 as stc
import base64
import time
import tempfile
from pathlib import Path
from PIL import Image


# OpenAI client (new SDK style)
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# åˆå›è¡¨ç¤ºã®ãƒ•ãƒ©ã‚°ã¨ãƒšãƒ¼ã‚¸ç•ªå·
if "show_intro" not in st.session_state:
    st.session_state.show_intro = True
if "intro_page" not in st.session_state:
    st.session_state.intro_page = 1

# HTMLã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªç”»åƒï¼ˆç”»åƒã‚’ãƒœã‚¿ãƒ³ã«è¦‹ç«‹ã¦ã‚‹ï¼‰
def clickable_image(image_path, next_action):
    with open(image_path, "rb") as f:
        data = f.read()
    b64 = base64.b64encode(data).decode()
    image_html = f"""
    <a href="?next={next_action}">
        <img src="data:image/png;base64,{b64}" 
             style="display: block; margin-left: auto; margin-right: auto; width: 100%; max-width: 1800px; border-radius: 20px; cursor: pointer;" />
    </a>
"""
    st.markdown(image_html, unsafe_allow_html=True)

# ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèª
query_params = st.experimental_get_query_params()
next_action = query_params.get("next", [None])[0]

# --- ã‚¤ãƒ³ãƒˆãƒ­è¡¨ç¤ºéƒ¨åˆ† ---
if st.session_state.show_intro:
    if next_action == "page2":
        st.session_state.intro_page = 2
        st.experimental_set_query_params()
    elif next_action == "main":
        st.session_state.show_intro = False
        st.experimental_set_query_params()
        st.experimental_rerun()

    # ãƒšãƒ¼ã‚¸ã”ã¨ã®å‡¦ç†
    if st.session_state.intro_page == 1:
        clickable_image("start.png", "page2")  # 1ãƒšãƒ¼ã‚¸ç›®
        st.stop()
    elif st.session_state.intro_page == 2:
        clickable_image("home.png", "main")   # 2ãƒšãƒ¼ã‚¸ç›®ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§æœ¬ç·¨ï¼‰
        st.stop()


st.set_page_config(page_title="ãƒãƒ–ãƒ€ãƒãƒ©ãƒƒãƒ—ãƒãƒˆãƒ«", layout="wide")
st.title("ğŸ¤ ãƒãƒ–ãƒ€ãƒãƒ©ãƒƒãƒ—ãƒãƒˆãƒ«")

# BGMå†ç”Ÿ
audio_path1 = 'sample_music.mp3'

audio_placeholder = st.empty()
with open(audio_path1, "rb") as file_:
    contents = file_.read()

audio_str = "data:audio/ogg;base64,%s" % (base64.b64encode(contents).decode())
audio_html = f"""
    <audio id="bgm-player" autoplay loop>
        <source src="{audio_str}" type="audio/ogg">
        Your browser does not support the audio element.
    </audio>
    <script>
        const audio = document.getElementById("bgm-player");
        audio.volume = 0.5;
    </script>
"""

audio_placeholder.empty()
time.sleep(0.5)
audio_placeholder.markdown(audio_html, unsafe_allow_html=True)

col1, col_chat, col2 = st.columns([1.0, 2, 1.0])

# å·¦å´
with col1:
    image_hidari = Image.open("hidari_icon.png")
    st.image(image_hidari, caption='', use_column_width=True)
    
    st.header("å·¦ã®ãƒãƒ–ãƒ€ãƒ")
    left_name = st.text_input("åå‰ï¼ˆå·¦ï¼‰", value="", placeholder="æ—ã•ã‚“")
    left_traits = [
        st.text_input("é­…åŠ›â‘ ï¼ˆå·¦ï¼‰", value="", placeholder="æ¸…æ½”æ„ŸãŒã‚ã‚‹"),
        st.text_input("é­…åŠ›â‘¡ï¼ˆå·¦ï¼‰", value="", placeholder="å‰£é“ãŒå¼·ã„"),
        st.text_input("é­…åŠ›â‘¢ï¼ˆå·¦ï¼‰", value="", placeholder="è‹±æ¤œäºŒç´š")
        # st.text_input("é­…åŠ›â‘ ï¼ˆå·¦ï¼‰", "æ¸…æ½”æ„ŸãŒã‚ã‚‹"),
        # st.text_input("é­…åŠ›â‘¡ï¼ˆå·¦ï¼‰", "å‰£é“ãŒå¼·ã„"),
        # st.text_input("é­…åŠ›â‘¢ï¼ˆå·¦ï¼‰", "è‹±æ¤œäºŒç´š")
    ]

# å³å´
with col2:
    image_migi = Image.open("migi_icon.png")
    st.image(image_migi, caption='', use_column_width=True)
    st.header("å³ã®ãƒãƒ–ãƒ€ãƒ")
    right_name = st.text_input("åå‰ï¼ˆå³ï¼‰", value="", placeholder="ç´°è°·ã•ã‚“")
    right_traits = [
        
        st.text_input("é­…åŠ›â‘ ï¼ˆå³ï¼‰", value="", placeholder="ãƒ‡ã‚¶ã‚¤ãƒ³ãŒã†ã¾ã„"),
        st.text_input("é­…åŠ›â‘¡ï¼ˆå³ï¼‰", value="", placeholder="å˜˜ãŒã†ã¾ã„"),
        st.text_input("é­…åŠ›â‘¢ï¼ˆå³ï¼‰", value="", placeholder="å‹é”ãŒå¤šã„")

        # st.text_input("é­…åŠ›â‘ ï¼ˆå³ï¼‰", "ãƒ‡ã‚¶ã‚¤ãƒ³ãŒã†ã¾ã„"),
        # st.text_input("é­…åŠ›â‘¡ï¼ˆå³ï¼‰", "å˜˜ãŒã†ã¾ã„"),
        # st.text_input("é­…åŠ›â‘¢ï¼ˆå³ï¼‰", "SFãŒå¥½ã")
    ]

st.markdown("---")

# STARTãƒœã‚¿ãƒ³
with col_chat:
    
    if st.button("ğŸ”¥ ãƒ©ãƒƒãƒ—ãƒãƒˆãƒ« STARTï¼",use_container_width=True):
        if not left_name or not right_name or "" in left_traits or "" in right_traits:
            st.warning("âš ï¸ ã™ã¹ã¦ã®é …ç›®ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼")
            st.stop()
        with st.spinner("ãƒ©ãƒƒãƒ—ãƒãƒˆãƒ«ä¸­... ğŸ¤ğŸ’¥"):
            prompt = f"""
ä»¥ä¸‹ã®2äººãŒãƒãƒ–ãƒ€ãƒãƒ©ãƒƒãƒ—ãƒãƒˆãƒ«ã‚’è¡Œã„ã¾ã™ã€‚
ãƒ©ãƒƒãƒ—ã¯åˆè¨ˆ4ã‚¿ãƒ¼ãƒ³ã§ã€äº¤äº’ã«2å›ãšã¤ç™ºè¨€ã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯å¿…ãšä»¥ä¸‹ã®å½¢ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆãƒ©ãƒƒãƒ‘ãƒ¼ã®åå‰ã¨ãƒ©ãƒƒãƒ—ã¯åˆ¥è¡Œï¼‰ï¼š

{left_name}
ãƒ©ãƒƒãƒ—1ï¼ˆ3ã€œ5è¡Œï¼‰
{right_name}
ãƒ©ãƒƒãƒ—2ï¼ˆ3ã€œ5è¡Œï¼‰
{left_name}
ãƒ©ãƒƒãƒ—3ï¼ˆ3ã€œ5è¡Œï¼‰
{right_name}
ãƒ©ãƒƒãƒ—4ï¼ˆ3ã€œ5è¡Œï¼‰

ã€ãƒãƒˆãƒ«å‚åŠ è€…ã€‘
- {left_name} ã®é­…åŠ›: {', '.join(left_traits)}
- {right_name} ã®é­…åŠ›: {', '.join(right_traits)}

ç›¸æ‰‹ã‚’ãƒ‡ã‚£ã‚¹ã‚Šã¤ã¤ã‚‚ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚„å€‹æ€§ã‚’å‡ºã—ã¦ãƒ©ãƒƒãƒ—ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
å‰æŒ¯ã‚Šãªã©ã¯å…¨ãå¿…è¦ãªãã€ä¸Šã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æº€ãŸã—ãŸ4ã¤ã®ãƒ•ãƒ­ãƒ¼ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
1ãƒ•ãƒ­ãƒ¼ã¯50æ–‡å­—
"""

            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ©ãƒƒãƒ—ãƒãƒˆãƒ«ã®å¸ä¼šè€…ã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.9,
                max_tokens=500
            )

            rap_battle = response.choices[0].message.content

            # ãƒ©ãƒƒãƒ—éƒ¨åˆ†ã ã‘æŠ½å‡º
            lines = rap_battle.strip().split("\n")
            flows = []
            buffer = []
            for line in lines:
                if line.strip() in [left_name, right_name]:
                    if buffer:
                        flows.append("\n".join(buffer).strip())
                        buffer = []
                else:
                    buffer.append(line)
            if buffer:
                flows.append("\n".join(buffer).strip())
            # å¤‰æ•°ã«æ ¼ç´
            left_flow_1 = flows[0]
            right_flow_1 = flows[1]
            left_flow_2 = flows[2]
            right_flow_2 = flows[3]
            all_flows = [left_flow_1, right_flow_1, left_flow_2, right_flow_2]


            voice_placeholder = st.empty()

                        # --- 2. éŸ³å£°ç”Ÿæˆ ---
            speech_path = Path(f"speech.mp3")
            with client.audio.speech.with_streaming_response.create(
                model="tts-1",
                voice="nova",  # ã‚‚ã—ãã¯ "coral"
                input="\n".join(all_flows),
                instructions="Speak in a cheerful and positive tone.",
            ) as response:
                response.stream_to_file(speech_path)

            # --- 3. éŸ³å£°å†ç”Ÿ ---
            with open(speech_path, "rb") as file_:
                contents = file_.read()
            voice_str = "data:audio/ogg;base64,%s" % (base64.b64encode(contents).decode())

            voice_html = f"""
                <audio autoplay=True>
                <source src="{voice_str}" type="audio/ogg">
                Your browser does not support the audio element.
                </audio>
            """

            voice_placeholder.markdown(voice_html, unsafe_allow_html=True)

            for idx, flow in enumerate(all_flows):
                # --- 1. ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º ---
                if idx % 2 == 0:
                    st.image(image_hidari, caption='', width=50)
                else:
                    st.image(image_migi, caption='', width=50)

                st.markdown(f"<p style='font-size: 40px;'>{flow}</p>", unsafe_allow_html=True)
                st.markdown("---")

                time.sleep(13)



            # å‹æ•—åˆ¤å®š
            st.markdown("### ğŸ† å‹æ•—åˆ¤å®š")

            judge_prompt = f"""
ä»¥ä¸‹ã®ãƒ©ãƒƒãƒ—ãƒãƒˆãƒ«ã®å†…å®¹ã‚’èª­ã‚“ã§ã€ã©ã¡ã‚‰ã®ãƒ©ãƒƒãƒ‘ãƒ¼ãŒå‹ã£ãŸã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
å‹è€…ã®åå‰ï¼ˆ{left_name} ã¾ãŸã¯ {right_name}ï¼‰ã¨ã€ãã®ç†ç”±ã‚’ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ãã®éš›ã€ä¸¡æ–¹ã‚’è¤’ã‚ã€ä¸¡æ–¹ãŒå„ªã‚ŒãŸäººé–“ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚

ã€ãƒ©ãƒƒãƒ—ãƒãƒˆãƒ«å†…å®¹ã€‘
{rap_battle}
"""

            judge_response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ©ãƒƒãƒ—ãƒãƒˆãƒ«ã®å¯©æŸ»å“¡ã§ã™ã€‚"},
                    {"role": "user", "content": judge_prompt}
                ],
                temperature=0.7,
                max_tokens=500
            )

            judge_result = judge_response.choices[0].message.content
            st.success(judge_result)
